<html>

<meta charset="UTF-8">

<head>
<link rel="stylesheet" href="style.css">
  <title>...</title>
</head>

<body>
<div class="main_container">

<h1>
  It's a website! <a href="index.html">Home</a>
  <div class="headshot"><img class="headshot" src="media/headshot_feb2024_sm.jpg"></div>
</h1>

<div style="width:100%; height:1px;">
    &nbsp;
</div>

<h2>
Project ideas
</h2>
This is an unstructured list of project ideas; big or small. 
Is it novel? Did someone do it already? I don't know! But they seem fun.

<div class="card">
<h3>Newton's method</h3>

A few years back in having fun with Newton's method I found a publication by 
a Michael Hurley highlighting/doing some work showing Newton's method 
applied to certain polynomials results in periodic orbits:

<ul>
    <li> <a href="https://epubs.siam.org/doi/abs/10.1137/0515020" target=_blank>Hurley and Martin 1984, Newtonâ€™s Algorithm and Chaotic Dynamical Systems</a>
    <li> <a href="https://www.jstor.org/stable/2000461" target=_blank>Attracting Orbits in Newton's Method</a>
    <li> <a href="https://www.cambridge.org/core/journals/ergodic-theory-and-dynamical-systems/article/multiple-attractors-in-newtons-method/B0258F571B372C2EECD20B1E78B4F85E" target=_blank>Hurley 1986, Multiple attractors in Newton's method</a> (general)
</ul>

It would be interesting to explore through these papers, do computational experiments, explore the size of basins and if/how large they can be made.
</div>


<div class="card">
<h3>Apertus</h3>
<i>04 SEP 2025</i><p>

Anything with this fully open large language model, fairly competitive 
with semi-open models of the same size (e.g. LLaMa), 
see <a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/09/press-release-apertus-a-fully-open-transparent-multilingual-language-model.html" target="_blank">announcement</a>
</div>

<div class="card">
<h3>Sparse identification of nonlinear dynamics; incompressible fluids.</h3>
<i>25 AUG 2025</i><p>

We recently implemented linearly-constrained sparse regression for 
discovery of differential equations from data for compartmental models. 
Incompressible fluids in two or three dimensions come with a direct 
divergence-free constraint such as $u_x + u_y = 0$. Take data from a 
few solvers for different fluid behaviors/domains that go past 
Stokes or Navier-Stokes and see if/when the equations with 
incompressibility can be discovered.

</div>

<div class="card">
<h3>Reservoir computing. What gives?</h3>
<i>25 AUG 2025</i><p>

Reservoir computing is a magic tool in the dynamical systems/physics community 
for fitting to nonlinear dynamics by combining an arbitrary high-dimensional 
nonlinear embedding and <b>only</b> fitting the linear output weights to the 
timeseries of interest. What gives? Implement it and study it.

</div>

<div class="card">
<h3>Robust derivative estimation; SNR parameter studies</h3>
<i>25 AUG 2025</i><p>

Methods for robustly estimating derivative values in time series 
given noisy observations are useful for discovery of differential 
equations from data. Like most things, these aren't "golden bullets" 
for which any type of data and any type/magnitude of noise can be handled.
Do a parameter sweep of signal-to-noise ratio for various derivative 
estimation tools and understand relative performance; dependence 
on iid vs non-iid noise.

See e.g. <a href="https://onlinelibrary.wiley.com/doi/10.5402/2011/164564" target="_blank">Chartrand; <i>Numerical Differentiation of Noisy, Nonsmooth Data</i></a>
</div>

<div class="card">
<h3>Sparse identification of DEs with nonlinear constraints.</h3>
<i>25 AUG 2025</i><p>

In the context of sparse identification of differential equations from 
data, I have been developing constrained versions in the context of 
compartment models (sum of compartment values adds to a constant) which is a 
linear/subspace constraint on the derivative values, which enables 
efficient projection based implementations. What does this look like for 
nonlinear constraints? For example, the linear or nonlinear pendulum. 
Are there others? How does this work in practice? What about 
simultaneously finding the conservation as well as the law?
</div>


<div class="card">
<h3>Graph connectivity/properties of dual graph produced by 
the tilings produced by aperiodic monotiles.</h3>
<i>25 AUG 2025</i><p>

Periodic tilings of the plane must produce patterned graphs. 
For example, rectangular tiles produce a grid. More sophisticated, 
or two-tile tilings, produce graphs which have predictable degrees 
sequences/proportions/conditional knowledge. What does this look like 
for the aperiodic monotile? Develop code to produce sequences of 
finite tilings and study properties of the graphs. Understand asymptotic 
behavior as the sequence of finite tilings grows.
</div>


<div class="card">
<h3>Tokens by language.</h3>
<i>25 AUG 2025</i><p>

Generating tokens for the purpose of training next-token predictors; 
language models; etc must be a language-dependent thing. How does this 
look exactly? What aspects of a language play, agree/disagree with aspects 
of languages that people understand? For example, consonant-heavy languages.

</div>


<div class="card">
<h3>Reading group, bayesian scientific computing</h3>
<i>25 AUG 2025</i><p>

Doing a reading group and/or setting up a collection of scientific questions 
for which Bayesian as well as "traditional" applied math/numerical methods 
can be applied. Understanding similarities in differences at the algorithm
or per-iteration of actual implementation level. Understanding differences 
betweenn theoretical aspects (the ability to sample infinitely from a 
probability density vs being handed a finite dataset; i.e. empirical density).
</div>

<div class="card">
<h3>Math data science: predicting loss curves for SGD with Volterra integral equations</h3>
<i>12 AUG 2025</i> <p>

In a relatively recent paper by <a href="https://arxiv.org/abs/2102.04396" target="_blank">Paquette et al (2021)</a>, they derived a cute formula for the normed 
error of in "training" the weights in an ordinary least squares problem 
using stochastic gradient descent (SGD). What is "cute" is that the formula 
relies on an peculiar "Volterra integral equation" which involves
an integrand relies on the spectrum (set of eigenvalues) of the Hessian 
of the data matrix alone. What they produce in this line of work is actual 
loss curves as a function of epoch, and the smooth curves coming from this 
theory which overlap. <br>

The catch is that this is done for an ordinary least squares problem; 
not a nonlinear problem. In other words, most neural networks do not 
fit into this theory. It would be interesting to see if the theorized curve 
pans out anyway, or what "inroads" can be made in seeing this theory work for 
nonlinear problems.

The path would look like:
<ol>
  <li> Set up an SGD process for solving an ordinary least squares problem 
  using the "streaming" SGD process required.
  <li> Set up tools to compute this Hessian (or Gramian) of the data matrix. 
  <li> Set up tools to compute this integral and identify if this works for 
  e.g. synthetic data (rows sampled from a multivariate normal); or MNIST; or CIFAR-10.
  <li> These bounds/predictions are for ordinary least squares. Set up a 
  shallow neural network with a single ReLU activation on the output, for 
  example, and see how the results compare. 
  <li> Branch out. Are there extremes where this theory succeeds or fails  
  for general nonlinear models, convolutional nets, etc?
</ol>
</div>

<div class="card">
<h3>Simplified fire containment.</h3>
<i>06 JAN 2025</i> <p>

In an idealized scenario in two dimensions where fire obeys 
e.g. a diffusion equation (or a Fisher equation or any other PDE you like), 
supposing you have a tractor that digs a containment ditch 
starting at (1,0) and the fire begins with point source at (0,0). 
You must dig the ditch such that 
fire does not escape.
<ul> 
<li> What is the optimal path (x(t), y(t)) which minimizes the burn area? 
What shape is the bounded region?
<li> What is an appropriate way to handle this problem numerically? 
What does a "greedy" choice of direction u mean when the constraint is only that 
x(t)=1, y(t)=0 for t>T for some T?
<li> In nondimensionalizing (which I've done a little bit of this by starting the 
tractor at (1,0)), what are the relevant quantities? If the problem is normalized 
so that T=1, this is... a PDE constrained boundary value problem for the path 
(x(t), y(t))?
<li> Supposing the fire travels at speed 1, what is a necessary requirement 
for the fire to be containable? Does the tractor need speed 1+\varepsilon 
for any \varepsilon>0? Is the fire containable with tractor speed 1 in any situation?
</div>

<div class="card">
<h3>Analysis of trends in SIAM conferences.</h3>
<i>23 DEC 2024</i> <p>

This falls under what sometimes is called "science of science."

It would be interesting to do topic analysis, NLP, etc relating to 
SIAM conferences, as measured by title and/or abstract of 
posters and/or oral presentations.
One example of a summary (titles/affiliations only) is here: 
<a href="https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=80900" target="_blank">a poster session from MDS24</a>; this format has more or less persisted for years.
<ul>
  <li> (semi-optional): Familiarize with data mining tools; extract as broad/long 
  a sample of information from conferences as possible
  <li> (alternative): Communicate with SIAM and obtain tabulated data directly;
  <li> Beginning with tabulated data, extract features/topics. Quantify trends.
  <li> Understand representation in presenters/affiliations over time; 
  spatially distributed; relating to grant awards/prestige.
</ul>
</div>

<div class="card">
<h3>Mitigating bias in a classifier.</h3>
<i>23 DEC 2024</i> <p>

Since an increasing number of decisions are being quantified and evaluated 
based on numerical scores (decisions about recidivism, mortgage applications, 
etc), it is important to understand whether an ML model can "do both" 
the task at hand and also avoid disproportionate impact based on 
protected classes (as defined by Title IX, for example: race/ethnicity, sex, 
gender, and so on). Studying this with real data is challenging. 
I don't know what data exists from a practical case-study perspective. 
But one can treat it as a "toy problem" and go from there:
<ol>
    <li> Generate two sets of synthetic data in R^n; with two sets of 
    labels; the primary, and a "hidden" secondary (an ordinal protected class).
    <ol>
        <li> Side-question throughout: how/when should the secondary label be 
        sampled; should the questions be evaluated based on correlations in 
        the primary/secondary label; and so on.
    </ol>
    <li> Use a toolbox such as sklearn and train a classifier to predict the 
    primary label. Apply a usual ML process (train/test; evaluation of 
    confusion matrix; AUCROC for binary).
    <li> After the model is trained, evaluate the performance on the train/test 
    data <b>separately</b> depending on the "hidden" label. Investigate and/or 
    set up a small collection of case studies where the "side-question" above 
    produces no/some/maximal differences based on the hidden label.
    <li> Move to a model-training framework that allows for mixed-objective 
    optimization (for example, PyTorch) which has a loss function on both 
    the primary, regularized by the secondary, with a regularization parameter.
    <li> Study the landscape of parameter space in those settings. How/when can 
    a primary objective (e.g. classification rates, balanced success) remain 
    "virtually" the same while allowing for improvements on disproportionate 
    impact on the hidden labels?
</ol>
</div>

<div class="card">
<h3>Fitting nonlinear ODEs to data.</h3>
<i>23 DEC 2024</i> <p>

Given data from a nonlinear ODE suspected to be the 
(say, the <a href="https://en.wikipedia.org/wiki/Lorenz_system#Overview" target="_blank">Lorenz system</a>), 
with data on the interval [0,T], the question is "how much" data is needed 
to discover the underlying law. 
<ol>
    <li> Choose a scheme to approximate/replace the derivatives with numerical values 
    at each data point;
    <li> Set and up a linear least squares of polynomial ODE systems;
    <li> Re-apply solved coefficients and re-simulate the system
    <li> Estimate error in various senses:
        <ol>
            <li> In coefficient space; a 2-norm error of all coefficients used in the fit.
            <li> In simulated space; a pointwise 2-norm error between data points;
            <li> In simulated space; a manifold distance (e.g. Wasserstein or other "manifold metric")
        </ol>
    <li> Evaluate conditioning of the problem as a function of T.
    <li> Evaluate anything else as a function of T.
    <li> Compare to traditional measures of chaotic behavior.
</ol>
</div>

<div class="card">
<h3>Sparsity-based fitting of nonlinear ODEs for compartmental models.</h3>
<i>23 DEC 2024</i> <p>

In the context of <a href="https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology" target=_blank>compartmental models</a> (SIR models and its friends), there is an 
enormous zoo of models with various underlying assumptions about the pathogen and 
the behavior of the population. The question is if 
we can use an "overkill" compartmental model and use sparsity-promoting 
regularizers/optimization methods to automatically select and identify parameters 
in the "correct" compartmental model(s) given a set of data. 

This begins/assumes familiarity with the process in the Lorenz problem above...
<ol>
    <li> Simulate data from a 1D logistic model S' = -k S(1-S), and see 
    if the coefficients can be successfully fit and produce qualitatively 
    similar behavior to the underlying law.
    <li> Simulate data from a vanilla SIR model and see if the coefficients can be 
    fit in a similar fashion. 
    <li> Investigate a tool (e.g. PyTorch, sklearn) which have tools for L1-regularized
    least-squares problems (e.g. elastic net in sklearn; pytorch may allow for 
    extending the work to uncharted territory more easily)
    <li> <b>Side-branch:</b> do a "conservation-based" regularization where 
    conservation laws are automatically enforced for compartmental models. 
    For example, dS/dt and dI/dt have equal-but-opposite terms on an SI term; 
    hence coefficients fit should be exactly equal and opposite.
    <li> Expand the model fit to e.g. an SEIR model. Have two ground-truth 
    simulations; one which includes E; the other which doesn't. 
    Revisit regularization to see if the "E" compartment can be bypassed. 
    May need a careful formulation to avoid ill-conditioning (coefficients 
    going to infinity?)
    <li> If successful, go nuts with the original question.
    <li> Apply the methods to real data. Evaluate.
</ol>
</div>


<div class="card">
<h3>"Florida Man"</h3>
<i>23 DEC 2024</i> <p>

Is the <a href="https://en.wikipedia.org/wiki/Florida_Man" target=_blank>Florida Man</a> 
meme quantitatively observable?

This falls under natural language processing (NLP). If you aren't familiar 
with "Florida Man," read the link above. Basically, bizarre crimes/arrest 
reports seem to disproportionately come from Florida (a variation is with Ohio). 
The question is whether one can apply any number of ideas from NLP to 
measure, state-by-state, prevalence of news reports of crime.
<ol>
    <li> Identify an open-data, queryable news service;
    <li> Apply naive criteria (does an article include a set of words of interest 
    such as "arrested", "felony", etc), to measure a time sreies of raw counts 
    of such data, state-by-state
    <li> Access and apply time series data of state populations and state 
    socioeconomic characteristics (population, income, wealth, education) to 
    normalize
    <li> Visualization; multivariate regression tools to extract a relevant metric
    per-state; what set of features sufficiently explain inter-state variability?
    <li> Revisit original criteria and apply more sophisticated tools 
    (topic modeling; clustering; sentiment analysis; etc)
</ol>
</div>

<div class="card">
<h3>Violin shapes and shape optimization.</h3>
<i>23 DEC 2024</i> <p>

A good friend recently visited the Musical Instrument Museum in 
Phoenix, AZ, and saw a curious gometric diagram of an idealized 
violin due to Antonio Bagatella, constructed with a 
collection of circular sectors. 
<ol>
    <li> Identify a tool (or make it) to define and draw a collection of 
    circular sectors, identify their intersections, and sketch. 
    That is, the center, radius, and interval of angles are needed.
    <li> Begin by attempting to define a simple closed region by a 
    collection of these. 
    <li> Discretize and pass off this object to a finite-element (or other 
    numerical tool) to find the spectrum/vibrational modes of the 2D object.
    <li> Modularize and expand the tool to pass a set of (center, radii, 
    sectors) as the input; with output the frequency.
    <li> Pass this function onto a black-box optimizer; see if shapes can be 
    designed to fit a prescribed spectrum. See if the prototypical violin shape can be 
    "rediscovered."
</old>
</div>

<div class="card">
<h3>Fruit-fly connectome data; simulation</h3>
<i>23 DEC 2024</i> <p>

"Recently," an entire fruit-fly connectome (what neurons connect to which) 
got mapped and open-sourced; see <a href="https://www.npr.org/2024/10/02/nx-s1-5124734/fruit-fly-brain-connectome-neurons" target=_blank>NPR</a>; <a href="https://www.nature.com/articles/s41586-024-07968-y" target=_blank>network summary statistics paper</a>; <a href="https://flywire.ai/" target=_blank>FlyWire connectome website</a>.

Inspired by questions about how/whether artificial neural nets mimick 
behavior of neurons, the question is whether this can be seen with 
"real" connectivity. 

<ol>
    <li> Access/load/parse connectome data.
    <li> Do network visualization/summary statistics on all or some subset of the data.
    <li> Access/learn tools for loading/training artificial neural networks 
    <li> Take some subset of the fruit fly connectome. With a choice of activation 
    functions, and a simple objective (maybe image classification or something else)
    with a <b>fixed NN structure</b>, see to what degree the ANN can succeed in 
    its task.
</ol>
</div>

</div>
</body>

</html>
